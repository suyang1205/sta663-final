{
 "metadata": {
  "name": "",
  "signature": "sha256:900ac37886b361a22a31802ee75ca8e44d54ecaca4857ac7d7747bcb75bf89bd"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Final Project: Bayesian Hierarchical Clustering\n",
      "----\n",
      "1. Background\n",
      "----\n",
      "\n",
      "Clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar to each other than to those in other groups. Algorithms of clustering includes connectivity based clustering (hierarchical clustering), centroid-based clustering(k-means clustering), distribution-based clustering, and density-based clustering. I mainly focus on hierarchical clustering.\n",
      "\n",
      "###  1.1 Traditional Hierarchical Clustering\n",
      "\n",
      "The traditional method for hierarchical clustering  is a bottomup agglomerative algorithm. It starts with each data point assigned to its own cluster and iteratively merges the two closest clusters together until all the data belongs to a single cluster. The nearest pair of clusters is chosen based on a given distance measure (e.g. Euclidean distance between cluster means, or distance between nearest points).\n",
      "\n",
      "###  1.2 Limitations of Traditional Hierarchical Clustering\n",
      "\n",
      "The tradtional hierarchical clustering method has many limitations:\n",
      "- The algorithm provides no guide to choosing the \u201ccorrect\u201d number of clusters or the level at which to prune the tree;\n",
      "- It is often dif\ufb01cult to know which distance metric to choose, especially for structured data such as images or sequences;\n",
      "- The traditional algorithm does not de\ufb01ne a probabilistic model of the data, so it is hard to ask how \u201cgood\u201da clustering is, to compare to other    models, to make predictions and cluster new data into an existing hierarchy.\n",
      "\n",
      "###  1.3 Bayesian Hierarchical Clustering\n",
      "\n",
      "To deal with those problems, I implement a novel algorithm for agglomerative hierarchical clustering using bayesian probabilistic model. Bayesian hierarchical clustering algorithm uses marginal likelihoods to decide which clusters to merge and to avoid over\ufb01tting. Basically it asks what the probability is that all the data in a potential merge were generated from the same mixture component, and compares this to exponentially many hypotheses at lower levels of the tree.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "----\n",
      "2. Algorithm and Implementation\n",
      "----\n",
      "\n",
      "###  2.1 Theories Behind the Algorithm\n",
      "\n",
      "The main di\ufb00erence between traditional hierarchical clustering method and bayesian hierarchical clustering is that bayesian hierarchical clustering uses a statistical hypothesis test to choose which clusters to merge.\n",
      "\n",
      "In considering each merge, two hypotheses are compared. The \ufb01rst hypothesis, which I will denote $H_1^k$, is that all the data in cluster $D_k$ were in fact generated independently and identically from the same probabilistic model, $p(x|\u03b8)$ with unknown parameters $\\theta$. To evaluate the probability of the data under this hypothesis we need to specify some priors over the parameters of the model, $p(\\theta|\\beta)$ with hyperparameters $\\beta$. We now have the ingredients to compute the probability of the data $D_k$ under $H_1^k$:\n",
      "\n",
      "$$p(D_k|H_1^k)= \\int{p(D_k|\\theta)p(\\theta|\\beta) d\\theta}=\\int{[\\prod_{x^{(i)}\\in D_k}p(x^{(i)}|\\theta)]p(\\theta|\\beta) d\\theta} $$\n",
      "\n",
      "This calculates the probability that all the data in $D_k$ were generated from the same parameter values assuming a model of the form $p(x|\\theta)$. This is a natural model-based criterion for measuring how well the data \ufb01t into one cluster. \n",
      "\n",
      "The alternative hypothesis to $H_1^k$ would be that the data in $D_k$ has two or more clusters in it(the probability of a data set under a tree (e.g. $p(D_i|T_i)$) is de\ufb01ned below.):\n",
      "$$p(D_k|H_2^k)=p(D_i|T_i)p(D_j|T_j)$$\n",
      " \n",
      "\n",
      "Combining the probability of the data under hypotheses $H_1^k$ and $H_2^k$, weighted by the prior that all points in $D_k$ belong to one cluster, $\\pi_k= p(H_1^k)$, we obtain the marginal probability of the data in tree $T_k$:\n",
      "$$p(D_k|T_k)=\\pi_k p(D_k|H_1^k)+(1-\\pi_k)p(D_i|T_i)p(D_j|T_j)$$\n",
      "\n",
      "\n",
      "Therefore, we can derive the posterior probability of the merged hypothesis $r_k$ using  Bayes rule:\n",
      "$$r_k=\\frac{\\pi_k p(D_k|H_1^k)}{\\pi_k p(D_k|H_1^k)+(1-\\pi_k)p(D_i|T_i)p(D_j|T_j)}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###  2.2 Bayesian Hierarchical Clustering Algorithm\n",
      "\n",
      "The general algorithm is shown below:\n",
      "\n",
      "**input:** data $D={x^{(1)},...x^{(n)}}$, model $p(x|\\theta)$, prior $p(\\theta|\\beta)$\n",
      "\n",
      "**inialize:** number of clusters $c=n$, and $D_i={x^{(i)}}$ for $i=1,2,...n$\n",
      "\n",
      "**while** $c>1$ **do:**\n",
      "\n",
      "&emsp;Find the pair $D_i$ and $D_j$ with the highest probability of the merged hypothesis $r_k$\n",
      "\n",
      "&emsp;Merge $D_k\\leftarrow D_i \\bigcup D_j$, $T_k\\leftarrow (T_i, T_j)$\n",
      "\n",
      "&emsp;Delete $D_i$ and $D_j$, $c\\leftarrow c-1$\n",
      "\n",
      "**end while**\n",
      "\n",
      "**output:** Bayesian mixture model where each tree node is a mixture component\n",
      "\n",
      "The tree can be cut at points where $r_k < 0.5$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 2.3 Implementation in Python\n",
      "\n",
      "#### 2.3.1 Data Structure\n",
      "In the clustering process, in order to store the information for each cluster, I define a **class** called **\"bicluster\"** to record the properties of each cluster(i.e. the data structure for cluster is **class**). The properties for the class **bicluster** includes:\n",
      "\n",
      "- left: one of the two clusters in the new cluster after merging\n",
      "\n",
      "- right: the other one of the two clusters in the new cluster after merging\n",
      "\n",
      "- point: all data points in the cluster\n",
      "\n",
      "- id:  id for each cluster\n",
      "\n",
      "- probability: the marginal probability of the data in the tree\n",
      "\n",
      "- d: a parameter used in calculating the prior that all data points in $D_k$ belongs to one cluster"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#define a class called \"bicluster\" to represent cluster and its properties\n",
      "class bicluster:\n",
      "    def __init__(self, point, left=None,right=None,probability=None,d=None,id=None):\n",
      "        self.left = left\n",
      "        self.right = right  \n",
      "        self.point = point   \n",
      "        self.id = id     \n",
      "        self.probability = probability\n",
      "        self.d=d"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### 2.3.2 Input\n",
      "\n",
      "The input for the clustering function contains two part: a **data set**  used for clustering, and a **function** which calculates the probability that all data in $D_k$ are generated independently and identically from a same distribution(i.e. $P(D_K|H_1^k)$). The choice for the distribution and prior are in the following part."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### 2.3.3 Choice fot the probablistic model and corresponding prior\n",
      "\n",
      "For the probablistic model and prior, I choose **Normal-Inverse-Wishart** and **Dirichlet-Multinomial** in the implememtation. Since they are both conjugate priors, I can calculate the integral:\n",
      "\n",
      "- Normal-Inverse-Wishart(multinormal data):\n",
      "\n",
      "$$\\Sigma \\sim IW(\\Sigma_0^{-1}, \\nu_0)$$\n",
      "$$\\mu \\sim N(\\mu_0, \\Sigma/\\kappa_0)$$\n",
      "\n",
      "The integral $p(D_k|H_1^k)= \\int{p(D_k|\\theta)p(\\theta|\\beta) d\\theta}$ is:\n",
      "\n",
      "$$p(D_k|H_1^k)= \\frac{1}{\\pi^{nd/2}}\\frac{{|\\Lambda_0|}^{\\nu_0/2}}{{|\\Lambda_n|}^{\\nu_n/2}} (\\kappa_0/\\kappa_n)^{d/2} \\frac{\\Gamma_d(\\nu_n/2)}{\\Gamma_d(\\nu_0/2)}$$\n",
      "\n",
      "where $\\mu_n=\\frac{\\kappa_0}{\\kappa_0+n}\\mu_0+\\frac{n}{\\kappa_0+n}\\bar{x}$, $\\kappa_n=\\kappa_0+n$, $\\nu_n=\\nu_0+n$, $\\Lambda_n=\\Lambda_0+\\sum_i ^{N} (x_i-\\bar{x})(x_i-\\bar{x})'+\\frac{\\kappa_0}{\\kappa_0+n}(\\bar{x}-\\mu_0)(\\bar{x}-\\mu_0)'$\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#function to calculate the marginal probability under the H1 hypothesis using Normal-Inverse-Wishart distribution\n",
      "import numpy as np\n",
      "from scipy.stats import multivariate_normal\n",
      "import scipy\n",
      "from scipy.special import gamma \n",
      "import random\n",
      "from math import sqrt  \n",
      "\n",
      "def marginal_likelihood_NIW(points):\n",
      "    n=len(points)\n",
      "    p=len(points[0])\n",
      "    mean_data = np.mean(points, axis=0)\n",
      "    sum_squares = np.sum([np.array(np.matrix(x - mean_data).T * np.matrix(x - mean_data)) for x in points], axis=0)\n",
      "    #parameters for the prior distribution\n",
      "    k0=3.0\n",
      "    v0=10\n",
      "    sigma0=np.eye(p)\n",
      "    mu0=[0]*p\n",
      "    #parameters for the posterior distribution\n",
      "    kn=k0+n\n",
      "    vn=v0+n\n",
      "    sigman=sigma0+sum_squares+(float(k0*n)/(k0+n))*np.dot((mean_data-mu0).reshape(p,1),(mean_data-mu0).reshape(1,p))\n",
      "    gammadn=reduce(lambda x,y: x*y, map(lambda x:gamma((vn+1-x)*0.5),range(1,p+1)))\n",
      "    gammad0=reduce(lambda x,y: x*y, map(lambda x:gamma((v0+1-x)*0.5),range(1,p+1)))\n",
      "    #calculate the marginal probability\n",
      "    mar_like=np.pi**(-p*n*0.5)*(np.linalg.det(sigma0))**(v0*0.5)*(k0/kn)**(0.5*p)*gammadn/(float(gammad0)*(np.linalg.det(sigman))**(vn*0.5))\n",
      "    \n",
      "    return mar_like"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- Dirichlet-Multinomial(multinomial data):\n",
      "$$p\\sim \\frac{\\Gamma(\\sum \\alpha_i)}{\\prod_i^{k} \\Gamma(\\alpha_i)} \\prod_i^{k}p_i^{\\alpha_i-1}$$\n",
      "\n",
      "The integral $p(D_k|H_1^k)= \\int{p(D_k|\\theta)p(\\theta|\\beta) d\\theta}$ is:\n",
      "\n",
      "$$p(D_k|H_1^k)=\\prod_i [\\frac{k!}{\\prod_c x_i,c!} \\frac{\\Gamma(\\alpha_0)}{\\prod_c \\Gamma(\\alpha_c)} \\frac{\\prod_c \\Gamma(\\alpha_c+x_i,c)}{\\Gamma(\\alpha_0+k)}]$$\n",
      "\n",
      "where $\\alpha_0=\\sum \\alpha_i$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#function to calculate the marginal probability under the H1 hypothesis using Dirichlet-Multinomial distribution\n",
      "def marginal_likelihood_DW(points):\n",
      "    p=len(points[0])\n",
      "    n=len(points)\n",
      "    #parameters for the prior distribution\n",
      "    alpha=[1.0/p]*p\n",
      "    alpha0=np.sum(alpha)\n",
      "    \n",
      "    def temp(yi):\n",
      "        sum=1.0\n",
      "        for i in range(0,p):\n",
      "            sum=sum*gamma(alpha[i]+yi[i])/(gamma(yi[i]+1)*gamma(alpha[i]))\n",
      "        return sum\n",
      "    #calculate the marginal probability\n",
      "    mar_like=reduce(lambda x,y: x*y, map(temp,points))*(gamma(p+1)*gamma(alpha0)/float(gamma(alpha0+p)))**n\n",
      "    \n",
      "    return mar_like"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### 2.3.4 Function for Bayesian Hierarchical Clustering"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#function used to store the clustering result\n",
      "def yezi(clust):\n",
      "    if clust.left == None and clust.right == None :\n",
      "        return [clust.id]\n",
      "    return yezi(clust.left) + yezi(clust.right)\n",
      " \n",
      "    \n",
      "#cluster function\n",
      "#parameter \"data\" is the data sset we want to cluster\n",
      "#parameter \"function\" is the function to calculate the marginal probability, which involves the choice of probablistic model and prior \n",
      "def bcluster(data,function) :\n",
      "    dim=len(data[0])#get the dimension of the data points\n",
      "    alpha=3\n",
      "    biclusters = [ bicluster(point = [data[i]], id = i ,probability=0.0000001,d=alpha) for i in range(len(data))] #initialize: each point is a cluster\n",
      "    flag = None;#record the id of the pair of clusters when merging\n",
      "    currentclusted = -1 #id for the new cluster\n",
      "    \n",
      "    #when there is only one data point in the data set\n",
      "    if len(biclusters) == 1:\n",
      "        clusters = [yezi(biclusters[i]) for i in range(len(biclusters))] \n",
      "        return biclusters,clusters\n",
      "    \n",
      "    #when there are more than one data points in the data set\n",
      "    while(len(biclusters) > 1) : \n",
      "        max_prob = 0; #initializa the maximum r_k\n",
      "        biclusters_len = len(biclusters)#number of the current clusters\n",
      "        \n",
      "        #find the pair of clusters that has the largest probability of merged hypothesis\n",
      "        for i in range(biclusters_len-1) :\n",
      "            for j in range(i + 1, biclusters_len) :\n",
      "                \n",
      "                temp_cluster_points= biclusters[i].point + biclusters[j].point#combine the two clusters into one cluster\n",
      "                P_H1=function(temp_cluster_points)#calculate the  probability that the points of the two clusters under H_1 hypothesis \n",
      "                #update the prior that all data points belong to one cluster\n",
      "                pi=float(scipy.misc.factorial(len(temp_cluster_points)-1))*alpha/(float(scipy.misc.factorial(len(temp_cluster_points)-1))*alpha+biclusters[i].d*biclusters[j].d)\n",
      "                marginal_prob=pi*P_H1+(1-pi)*biclusters[i].probability*biclusters[j].probability#calculate the marginal probabilty of data in tree T_k\n",
      "                r = pi*P_H1/marginal_prob#calculate the posterior probability of merged hypothesis using bayes rule\n",
      "                if r > max_prob :\n",
      "                    max_prob = r\n",
      "                    flag = (i,j)\n",
      "                    \n",
      "        #when to stop clustering     \n",
      "        if max_prob<0.5: \n",
      "            break\n",
      "            \n",
      "        bic1,bic2 = flag #update the flag\n",
      "        \n",
      "        newpoint = biclusters[bic1].point + biclusters[bic2].point #combine the points of two clusters into the new cluster\n",
      "        P_H1=function(newpoint)\n",
      "        #calculate the corresponding marginal probability for the new cluster\n",
      "        newprob=pi*P_H1+(1-pi)*biclusters[bic1].probability*biclusters[bic2].probability\n",
      "        newd=float(scipy.misc.factorial(len(newpoint)-1))*alpha+biclusters[bic1].d*biclusters[bic2].d\n",
      "        #construct the new culster using the information above\n",
      "        newbic = bicluster(point=newpoint, left=biclusters[bic1], right=biclusters[bic2], probability=newprob, d=newd,  id = currentclusted) \n",
      "        \n",
      "        currentclusted -= 1#update the id for the next cluster\n",
      "        #delete the two old clusters\n",
      "        del biclusters[bic2] \n",
      "        del biclusters[bic1]\n",
      "        \n",
      "        #add the new cluster into the biclusters\n",
      "        biclusters.append(newbic)\n",
      "        #record the clustering result\n",
      "        clusters = [yezi(biclusters[i]) for i in range(len(biclusters))] \n",
      "        \n",
      "    return biclusters,clusters"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "----\n",
      "3. Testing\n",
      "----\n",
      " \n",
      "In the unit test, I mainly test four things:\n",
      "\n",
      "- whether the number of clusters is larger than or equal to 1;\n",
      "\n",
      "- whether the number of clusters is smaller than or equal to the number of data points;\n",
      "\n",
      "- whether the algorithm can produce one cluster when all data points are exactly the same;\n",
      "\n",
      "- whether the algorithm can produce one cluster when there is only one data point."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The test code is in **test_Bathiecluster.py**.\n",
      "\n",
      "The test results are shown below:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "! py.test"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n",
        "platform linux2 -- Python 2.7.9 -- py-1.4.25 -- pytest-2.6.3\r\n",
        "\u001b[1m\r",
        "collecting 0 items\u001b[0m"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\u001b[1m\r",
        "collecting 4 items\u001b[0m\u001b[1m\r",
        "collected 4 items \r\n",
        "\u001b[0m\r\n",
        "test_Bayhiecluster.py "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "..\r\n",
        "\r\n",
        "\u001b[32m\u001b[1m=========================== 4 passed in 2.53 seconds ===========================\u001b[0m\r\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "----\n",
      "4. Optimization\n",
      "----\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this part, I would focus on the efficiency of the code(accuracy will be discussed later). \n",
      "\n",
      "In order to test the efficiency, I generate a sample data set(which is drawn from multinomial distribution):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sample=np.array(\n",
      "       [[1, 0, 4, 5],\n",
      "       [3, 2, 2, 3],\n",
      "       [0, 3, 6, 1],\n",
      "       [0, 2, 3, 5],\n",
      "       [1, 0, 7, 2],\n",
      "       [2, 0, 6, 2],\n",
      "       [0, 0, 5, 5],\n",
      "       [2, 1, 4, 3],\n",
      "       [3, 4, 1, 2],\n",
      "       [1, 3, 3, 3]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First, measure the time of function **bcluser** using the sample data set:\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%timeit -n 100 k,l=bcluster(sample,marginal_likelihood_DW) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "100 loops, best of 3: 23.1 ms per loop\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To improve efficiency, I try two method: **numba** and **cython**."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 4.1 Numba\n",
      "\n",
      "Numba gives  the power to speed up your applications with high performance functions written directly in Python with just a few annotations. The numba version of the function is shown below:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from numba import jit, int32, int64, float32, float64 \n",
      "import scipy.misc\n",
      "from numbapro import vectorize\n",
      "\n",
      "@jit\n",
      "\n",
      "def yezi(clust):\n",
      "    if clust.left == None and clust.right == None :\n",
      "        return [clust.id]\n",
      "    return yezi(clust.left) + yezi(clust.right)\n",
      "\n",
      "class bicluster:\n",
      "    def __init__(self, point, left=None,right=None,probability=None,d=None,id=None):\n",
      "        self.left = left\n",
      "        self.right = right  \n",
      "        self.point = point   \n",
      "        self.id = id     \n",
      "        self.probability = probability\n",
      "        self.d=d\n",
      "\n",
      "def marginal_likelihood_DW(points):\n",
      "    p=len(points[0])\n",
      "    n=len(points)\n",
      "    #parameters for the prior distribution\n",
      "    alpha=[1.0/p]*p\n",
      "    alpha0=np.sum(alpha)\n",
      "    \n",
      "    def temp(yi):\n",
      "        sum=1.0\n",
      "        for i in range(0,p):\n",
      "            sum=sum*gamma(alpha[i]+yi[i])/(gamma(yi[i]+1)*gamma(alpha[i]))\n",
      "        return sum\n",
      "    #calculate the marginal probability\n",
      "    mar_like=reduce(lambda x,y: x*y, map(temp,points))*(gamma(p+1)*gamma(alpha0)/float(gamma(alpha0+p)))**n\n",
      "    \n",
      "    return mar_like\n",
      "\n",
      "def bcluster_numba(data,function) :\n",
      "    dim=len(data[0])#get the dimension of the data points\n",
      "    alpha=3\n",
      "    biclusters = [ bicluster(point = [data[i]], id = i ,probability=0.0000001,d=alpha) for i in range(len(data))] #initialize: each point is a cluster\n",
      "    flag = None;#record the id of the pair of clusters when merging\n",
      "    currentclusted = -1 #id for the new cluster\n",
      "    \n",
      "    #when there is only one data point in the data set\n",
      "    if len(biclusters) == 1:\n",
      "        clusters = [yezi(biclusters[i]) for i in range(len(biclusters))] \n",
      "        return biclusters,clusters\n",
      "    \n",
      "    #when there are more than one data points in the data set\n",
      "    while(len(biclusters) > 1) : \n",
      "        max_prob = 0; #initializa the maximum r_k\n",
      "        biclusters_len = len(biclusters)#number of the current clusters\n",
      "        \n",
      "        #find the pair of clusters that has the largest probability of merged hypothesis\n",
      "        for i in range(biclusters_len-1) :\n",
      "            for j in range(i + 1, biclusters_len) :\n",
      "                \n",
      "                temp_cluster_points= biclusters[i].point + biclusters[j].point#combine the two clusters into one cluster\n",
      "                P_H1=function(temp_cluster_points)#calculate the  probability that the points of the two clusters under H_1 hypothesis \n",
      "                #update the prior that all data points belong to one cluster\n",
      "                pi=float(scipy.misc.factorial(len(temp_cluster_points)-1))*alpha/(float(scipy.misc.factorial(len(temp_cluster_points)-1))*alpha+biclusters[i].d*biclusters[j].d)\n",
      "                marginal_prob=pi*P_H1+(1-pi)*biclusters[i].probability*biclusters[j].probability#calculate the marginal probabilty of data in tree T_k\n",
      "                r = pi*P_H1/marginal_prob#calculate the posterior probability of merged hypothesis using bayes rule\n",
      "                if r > max_prob :\n",
      "                    max_prob = r\n",
      "                    flag = (i,j)\n",
      "                    \n",
      "        #when to stop clustering     \n",
      "        if max_prob<0.5: \n",
      "            break\n",
      "            \n",
      "        bic1,bic2 = flag #update the flag\n",
      "        \n",
      "        newpoint = biclusters[bic1].point + biclusters[bic2].point #combine the points of two clusters into the new cluster\n",
      "        P_H1=function(newpoint)\n",
      "        #calculate the corresponding marginal probability for the new cluster\n",
      "        newprob=pi*P_H1+(1-pi)*biclusters[bic1].probability*biclusters[bic2].probability\n",
      "        newd=float(scipy.misc.factorial(len(newpoint)-1))*alpha+biclusters[bic1].d*biclusters[bic2].d\n",
      "        #construct the new culster using the information above\n",
      "        newbic = bicluster(point=newpoint, left=biclusters[bic1], right=biclusters[bic2], probability=newprob, d=newd,  id = currentclusted) \n",
      "        \n",
      "        currentclusted -= 1#update the id for the next cluster\n",
      "        #delete the two old clusters\n",
      "        del biclusters[bic2] \n",
      "        del biclusters[bic1]\n",
      "        \n",
      "        #add the new cluster into the biclusters\n",
      "        biclusters.append(newbic)\n",
      "        #record the clustering result\n",
      "        clusters = [yezi(biclusters[i]) for i in range(len(biclusters))] \n",
      "        \n",
      "    return biclusters,clusters"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%timeit -n 100 k,l=bcluster_numba(sample,marginal_likelihood_DW)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "100 loops, best of 3: 25.1 ms per loop\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The performance of numba version  doesn't greatly reduce the time."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 4.2 Cython\n",
      "The Cython language is a superset of the Python language that additionally supports calling C functions and declaring C types on variables and class attributes. This allows the compiler to generate very efficient C code from Cython code.\n",
      "\n",
      "The Cython version of the function is shown below:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load_ext cythonmagic"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%cython\n",
      "import numpy as np\n",
      "cimport cython\n",
      "cimport numpy as np\n",
      "from scipy.special import gamma \n",
      "import scipy\n",
      "import scipy.misc\n",
      "\n",
      "@cython.boundscheck(False)\n",
      "@cython.wraparound(False)\n",
      "@cython.profile(False)\n",
      "\n",
      "cdef marginal_likelihood_DW_cython(np.ndarray[long,ndim=2] points):\n",
      "    cdef int p=len(points[0])\n",
      "    cdef int n=len(points)\n",
      "    cdef np.ndarray alpha=np.repeat(1.0/p,p)\n",
      "    cdef double alpha0=np.sum(alpha)\n",
      "    cdef double  mar_like\n",
      "    \n",
      "    def temp(np.ndarray[long,ndim=1] yi):\n",
      "        cdef int i\n",
      "        cdef double sum=1.0\n",
      "        for i in range(0,p):\n",
      "            sum=sum*gamma(alpha[i]+yi[i])/(gamma(yi[i]+1)*gamma(alpha[i]))\n",
      "        return sum\n",
      "    \n",
      "    mar_like=reduce(lambda x,y: x*y, map(temp,points))*(gamma(p+1)*gamma(alpha0)/float(gamma(alpha0+p)))**n\n",
      "    \n",
      "    return mar_like\n",
      "\n",
      "class bicluster:\n",
      "    def __init__(self, point, left=None,right=None,probability=None,d=None,id=None):\n",
      "        self.left = left\n",
      "        self.right = right  \n",
      "        self.point = point   \n",
      "        self.id = id     \n",
      "        self.probability = probability\n",
      "        self.d=d\n",
      "    \n",
      "def yezi(clust):\n",
      "    if clust.left == None and clust.right == None :\n",
      "        return [clust.id]\n",
      "    return yezi(clust.left) + yezi(clust.right)\n",
      "\n",
      "\n",
      "\n",
      "def bcluster_cython(np.ndarray[long,ndim=2] data) :\n",
      "   \n",
      "    cdef int dim=len(data[0])\n",
      "    cdef int alpha=3\n",
      "    cdef int currentclusted = -1 \n",
      "    cdef double P_H1, r, marginal_prob, pi, max_prob, newprob, newd\n",
      "    cdef int bic1, bic2, biclusters_len\n",
      "    cdef tuple flag = None\n",
      "    cdef np.ndarray temppoints, newpoints\n",
      "    \n",
      "    biclusters = [ bicluster(point = data[i], id = i ,probability=0.001,d=alpha) for i in range(len(data))] #initialize: each point is a cluster\n",
      "    \n",
      "    while(len(biclusters) > 1) : \n",
      "        max_prob = 0; \n",
      "        biclusters_len = len(biclusters)\n",
      "        for i in range(biclusters_len-1) :\n",
      "            for j in range(i + 1, biclusters_len) :\n",
      "                \n",
      "                #calculate P_H1: MC method\n",
      "                temppoints= np.vstack((biclusters[i].point , biclusters[j].point))\n",
      "                \n",
      "                #P_H1=marginal_likelihood_NIW(temp_cluster_points)\n",
      "                P_H1=marginal_likelihood_DW_cython(temppoints)\n",
      "                pi=float(scipy.misc.factorial(len(temppoints)-1))*alpha/(float(scipy.misc.factorial(len(temppoints)-1))*alpha+biclusters[i].d*biclusters[j].d)\n",
      "                marginal_prob=pi*P_H1+(1-pi)*biclusters[i].probability*biclusters[j].probability\n",
      "                r = pi*P_H1/marginal_prob\n",
      "                if r > max_prob :\n",
      "                    max_prob = r\n",
      "                    flag = (i,j)\n",
      "        \n",
      "        if max_prob<0.5: \n",
      "            break\n",
      "            \n",
      "        bic1,bic2 = flag \n",
      "        \n",
      "        newpoints = np.vstack((biclusters[bic1].point , biclusters[bic2].point)) #combine the points of two clusters into the new cluster\n",
      "        \n",
      "        #P_H1=marginal_likelihood_NIW(newpoint)\n",
      "        P_H1=marginal_likelihood_DW_cython(newpoints)\n",
      "        \n",
      "        newprob=pi*P_H1+(1-pi)*biclusters[bic1].probability*biclusters[bic2].probability\n",
      "        newd=float(scipy.misc.factorial(len(newpoints)-1))*alpha+biclusters[bic1].d*biclusters[bic2].d\n",
      "        newbic = bicluster(point=newpoints, left=biclusters[bic1], right=biclusters[bic2], probability=newprob, d=newd,  id = currentclusted) \n",
      "        currentclusted -= 1\n",
      "        \n",
      "        del biclusters[bic2] \n",
      "        del biclusters[bic1]\n",
      "        biclusters.append(newbic)\n",
      "        clusters = [yezi(biclusters[i]) for i in range(len(biclusters))] \n",
      "        \n",
      "    return biclusters,clusters"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%timeit -n 100 k,l=bcluster_cython(sample) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "100 loops, best of 3: 22.6 ms per loop\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The performance of cython version is not desirable; it doesn't greatly reduce the time. It may be due to the class bicluster. I couldn't find a way to transform the class bicluster into C type, which might be the main reason.  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "----\n",
      "5. Application and Comparison\n",
      "----\n",
      "\n",
      "### 5.1 Application to Simulated Data Set \n",
      "\n",
      "I simulate two data sets: \n",
      "\n",
      "- **Normal**: I simulate three data points from $N_2((0,0)',0.5I)$, 4 data points from $N_2((2,2)',0.5I)$, and 3 data points from $N_2((8,8)',0.5I)$.\n",
      "Then combine them into one data set called sample_NIW. This data set is supposed to be clustered into 3 clusters:(0,1,2,3) (4,5,6) (7,8,9).\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#sample multinormal data\n",
      "mean2=(0,0)\n",
      "cov2=0.5*np.eye(2)\n",
      "np.random.seed(4)\n",
      "data2=np.random.multivariate_normal(mean2,cov2,3)\n",
      "mean3=(2,2)\n",
      "cov3=0.5*np.eye(2)\n",
      "np.random.seed(5)\n",
      "data3=np.random.multivariate_normal(mean3,cov3,4)\n",
      "mean4=(8,8)\n",
      "cov4=0.5*np.eye(2)\n",
      "np.random.seed(6)\n",
      "data4=np.random.multivariate_normal(mean4,cov4,3)\n",
      "sample_NIW=np.concatenate((data3, data4,data2), axis=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- **Multinomial**: Data set sample_DW contains 3 identical data points [3, 3, 3, 3, 3, 3], and other 3 identical data points [9, 9, 0, 0, 0, 0]. This data set is supposed to be clustered into 2 clusters: (0,1,2) (3,4,5).\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#sample multinomial data\n",
      "\n",
      "d1=[[3, 3, 3, 3, 3, 3],[3, 3, 3, 3, 3, 3],[3, 3, 3, 3, 3, 3]]\n",
      "d2=[[9, 9, 0, 0, 0, 0],[9, 9, 0, 0, 0, 0],[9, 9, 0, 0, 0, 0]]\n",
      "sample_DW=np.concatenate((d1, d2), axis=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 5.2 Comparison\n",
      "\n",
      "#### 5.2.1 Bayesian Hierarchical Clustering\n",
      "\n",
      "Implement the algorithm using the two data sets:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Clustering result using normal data set:\n",
      "k1,l1=bcluster(sample_NIW,marginal_likelihood_NIW) \n",
      "l1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 16,
       "text": [
        "[[7, 8], [6, 4, 5, 0, 1, 2, 3, 9]]"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Clustering result using multinomial data set:\n",
      "k2,l2=bcluster(sample_DW,marginal_likelihood_DW) \n",
      "l2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 17,
       "text": [
        "[[1], [2], [0, 5, 3, 4]]"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For the normal data set sample_DW, function bcluster divides the data points into 2 clusters, and for the multinomial data set sample_DW, it divides the data points into three clusters. Though bayesian hierarchical clustering indeed correctly clusters part of the points, the overall performances are not good.  \n",
      "\n",
      "The main reason for the undesirable performance comes from the choice of probability distribution and its prior(including parameters). In the above two clustering processes, even though I use the correct probability distributions(multinomial and normal), it is hard to decide whether the priors are correct. In this case, I use conjugate priors for the sake of simplicity, but conjugate priors may not be the best one. What's more, the values of the parameters in the prior distributions are difficult to choose without additional information.\n",
      "\n",
      "More importantly, in some situations we don't know how the data is generated, so the choice of probability distribution is also a considerable problem.\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### 5.2.2 Traditional Hierarchical Clustering\n",
      "\n",
      "I first compare the result with traditional hierarchical clustering based on Euclidean distance.(The function using traditional hierarchical clustering is shown below)\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#define class tbicluster\n",
      "class tbicluster:\n",
      "    def __init__(self, vec, left=None,right=None,distance=0.0,id=None):\n",
      "        self.left = left\n",
      "        self.right = right  \n",
      "        self.vec = vec     \n",
      "        self.id = id     \n",
      "        self.distance = distance\n",
      "        \n",
      "#culstering function            \n",
      "def hcluster(data,n) :\n",
      "    \n",
      "    def Euclidean_distance(vector1,vector2):\n",
      "        length = len(vector1)\n",
      "        TSum = sum([pow((vector1[i] - vector2[i]),2) for i in range(len(vector1))])\n",
      "        SSum = sqrt(TSum)  \n",
      "        return SSum\n",
      "    \n",
      "    biclusters = [ tbicluster(vec = data[i], id = i ) for i in range(len(data)) ]\n",
      "    distances = {}\n",
      "    flag = None;\n",
      "    currentclusted = -1\n",
      "    while(len(biclusters) > n) : \n",
      "        min_val = 999999999999; \n",
      "        biclusters_len = len(biclusters)\n",
      "        for i in range(biclusters_len-1) :\n",
      "            for j in range(i + 1, biclusters_len) :\n",
      "                if distances.get((biclusters[i].id,biclusters[j].id)) == None:\n",
      "                    distances[(biclusters[i].id,biclusters[j].id)] = Euclidean_distance(biclusters[i].vec,biclusters[j].vec)\n",
      "                d = distances[(biclusters[i].id,biclusters[j].id)] \n",
      "                if d < min_val :\n",
      "                    min_val = d\n",
      "                    flag = (i,j)\n",
      "        bic1,bic2 = flag \n",
      "        newvec = [(biclusters[bic1].vec[i] + biclusters[bic2].vec[i])/2 for i in range(len(biclusters[bic1].vec))] \n",
      "        newbic = tbicluster(newvec, left=biclusters[bic1], right=biclusters[bic2], distance=min_val, id = currentclusted) \n",
      "        currentclusted -= 1\n",
      "        del biclusters[bic2] \n",
      "        del biclusters[bic1]\n",
      "        biclusters.append(newbic)\n",
      "        clusters = [yezi(biclusters[i]) for i in range(len(biclusters))] \n",
      "    return biclusters,clusters"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The result for the two data sets are shown below(given the correct number of clusters):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Culstering result using normal data set:\n",
      "tk1,tl1=hcluster(sample_NIW,3) \n",
      "tl1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "[[9, 7, 8], [1, 2, 0, 3], [6, 4, 5]]"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Clustering result using multinomial data set:\n",
      "tk2,tl2=hcluster(sample_DW,2) \n",
      "tl2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 20,
       "text": [
        "[[2, 0, 1], [5, 3, 4]]"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For both the normal data set sample_NIW and the multinomial data set sample_DW, traditional hierarchical clustering correctly clusters them into 3 and 2 groups with the right components.\n",
      "\n",
      "Although this alhorithm works fine in this case, it still have some concerns. The algorithm provides no guide to choosing the \u201ccorrect\u201d number of clusters or the level at which to prune the tree. It is often dif\ufb01cult to know which distance metric to choose, especially for structured data such as images or sequences. The traditional algorithm does not de\ufb01ne a probabilistic model of the data, so it is hard to ask how \u201cgood\u201d a clustering is, to compare to other models, to make predictions and cluster new data into an existing hierarchy.\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### 5.2.3 K-means Clustering\n",
      "\n",
      "Next, I use K-Means algorithm to cluster the two data sets. The K-Means algorithm clusters data by trying to separate samples in n groups of equal variance, minimizing a criterion known as the **inertia**  or within-cluster sum-of-squares. This algorithm requires the number of clusters to be specified. \n",
      "\n",
      "K-means is often referred to as Lloyd\u2019s algorithm. In basic terms, the algorithm has three steps. The first step chooses the initial centroids, with the most basic method being to choose k samples from the dataset X. After initialization, K-means consists of looping between the two other steps. The first step assigns each sample to its nearest centroid. The second step creates new centroids by taking the mean value of all of the samples assigned to each previous centroid. The difference between the old and the new centroids are computed and the algorithm repeats these last two steps until this value is less than a threshold. In other words, it repeats until the centroids do not move significantly.\n",
      "\n",
      "The function is shown below:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#kmeans\n",
      "import random\n",
      "\n",
      "def kmeanscluster(X, K):\n",
      "    \n",
      "    def cluster_points(X, mu):\n",
      "        clusters  = {}\n",
      "        for x in X:\n",
      "            bestmukey = min([(i[0], np.linalg.norm(x-mu[i[0]]))  for i in enumerate(mu)], key=lambda t:t[1])[0]\n",
      "            try:\n",
      "                clusters[bestmukey].append(x)\n",
      "            except KeyError:\n",
      "                clusters[bestmukey] = [x]\n",
      "        return clusters\n",
      "\n",
      "    def reevaluate(mu, clusters):\n",
      "        newmu = []\n",
      "        keys = sorted(clusters.keys())\n",
      "        for k in keys:\n",
      "            newmu.append(np.mean(clusters[k], axis = 0))\n",
      "        return newmu\n",
      " \n",
      "    def ifconverged(mu, oldmu):\n",
      "        return set([tuple(a) for a in mu]) == set([tuple(a) for a in oldmu])\n",
      "\n",
      "    # Initialize to K random centers\n",
      "    oldmu = random.sample(X, K)\n",
      "    mu = random.sample(X, K)\n",
      "    while not ifconverged(mu, oldmu):\n",
      "        oldmu = mu\n",
      "        # Assign all points in X to clusters\n",
      "        kclusters = cluster_points(X, mu)\n",
      "        # Reevaluate centers\n",
      "        mu = reevaluate(oldmu, kclusters)\n",
      "    return mu, kclusters"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The results for the two data sets are are not stable because of the first step--choosing the initial centroids randomly. Sometimes this algorithm could produce the correct clusters, while other times the results are wrong."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Besides, k-means suffers from various drawbacks:\n",
      "\n",
      "- Inertia makes the assumption that clusters are convex and isotropic, which is not always the case. It responds poorly to elongated clusters, or manifolds with irregular shapes.\n",
      "\n",
      "- Inertia is not a normalized metric: we just know that lower values are better and zero is optimal. But in very high-dimensional spaces, Euclidean distances tend to become inflated. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "----\n",
      "6. Future Work\n",
      "----\n",
      "\n",
      "- Efficiency: the optimization of bayesian hierarchical clustering has much to do with the data structure. In the function, I use class to define each cluster, but it is hard to be transformed into C. I will try to use a new data structure to define cluster, thus increasing efficiency.\n",
      "\n",
      "- Accuracy: as discuassed before, the inaccuracy of bayesian hierarchical clustering mainly derives from the choice of probability distribution and its prior. For the prior, I'll find some optimization algorithms to choose the best values for the parameters.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}